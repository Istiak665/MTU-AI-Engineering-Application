{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94daff48",
   "metadata": {},
   "source": [
    "### Step 1 — Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8999d7fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================\n",
    "# 1. Import Libraries\n",
    "# =============================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix,\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b53d72e",
   "metadata": {},
   "source": [
    "### Step 2 — Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d8ebef9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 569\n",
      "Total features: 30\n",
      "Target classes (0=malignant, 1=benign): {1: 357, 0: 212}\n"
     ]
    }
   ],
   "source": [
    "# =============================\n",
    "# 2. Load Dataset\n",
    "# =============================\n",
    "\n",
    "data = load_breast_cancer()\n",
    "\n",
    "X_all = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "y_all = pd.Series(data.target, name=\"target\")\n",
    "\n",
    "print(\"Total samples:\", X_all.shape[0])\n",
    "print(\"Total features:\", X_all.shape[1])\n",
    "print(\"Target classes (0=malignant, 1=benign):\", y_all.value_counts().to_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f35b0f",
   "metadata": {},
   "source": [
    "### Step 3 — Feature Analysis & Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c8b80173",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Feature correlations with target (absolute values):\n",
      "worst concave points    0.793566\n",
      "worst perimeter         0.782914\n",
      "mean concave points     0.776614\n",
      "worst radius            0.776454\n",
      "mean perimeter          0.742636\n",
      "worst area              0.733825\n",
      "mean radius             0.730029\n",
      "mean area               0.708984\n",
      "mean concavity          0.696360\n",
      "worst concavity         0.659610\n",
      "Name: target, dtype: float64\n",
      "\n",
      "Selected top 5 features based on correlation:\n",
      "['worst concave points', 'worst perimeter', 'mean concave points', 'worst radius', 'mean perimeter']\n"
     ]
    }
   ],
   "source": [
    "# =============================\n",
    "# 3. Feature Analysis & Selection\n",
    "# =============================\n",
    "\n",
    "\"\"\"\n",
    "Goal:\n",
    "Select 4-6 most relevant features.\n",
    "\n",
    "Strategy:\n",
    "We compute the correlation between each feature and the target variable.\n",
    "Then we select the top 5 features with highest absolute correlation.\n",
    "\"\"\"\n",
    "\n",
    "# Combine features and target into one DataFrame for correlation analysis\n",
    "df = X_all.copy()\n",
    "df[\"target\"] = y_all\n",
    "\n",
    "# Compute correlation matrix\n",
    "correlation_matrix = df.corr()\n",
    "\n",
    "# Extract correlation values with respect to target\n",
    "target_correlation = correlation_matrix[\"target\"].drop(\"target\")\n",
    "\n",
    "# Sort features by absolute correlation value (descending)\n",
    "sorted_features = target_correlation.abs().sort_values(ascending=False)\n",
    "\n",
    "print(\"\\nFeature correlations with target (absolute values):\")\n",
    "print(sorted_features.head(10))\n",
    "\n",
    "# Select top 5 most correlated features\n",
    "selected_feature_names = sorted_features.head(5).index.tolist()\n",
    "\n",
    "print(\"\\nSelected top 5 features based on correlation:\")\n",
    "print(selected_feature_names)\n",
    "\n",
    "# Create new dataset with only selected features\n",
    "X_selected = X_all[selected_feature_names]\n",
    "y_selected = y_all"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6fde397",
   "metadata": {},
   "source": [
    "### Step 4 — Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "42b71699",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train/Test Split Summary:\n",
      "Training samples: 455\n",
      "Testing samples: 114\n",
      "Number of features used: 5\n",
      "\n",
      "Training class distribution:\n",
      "target\n",
      "1    285\n",
      "0    170\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Testing class distribution:\n",
      "target\n",
      "1    72\n",
      "0    42\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# =============================\n",
    "# 4. Train-Test Split\n",
    "# =============================\n",
    "\n",
    "\"\"\"\n",
    "Goal:\n",
    "Split the selected dataset into training (80%) and testing (20%).\n",
    "\n",
    "Important:\n",
    "- stratify=y_selected ensures class distribution remains balanced.\n",
    "- random_state=42 ensures reproducibility.\n",
    "\"\"\"\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_selected,\n",
    "    y_selected,\n",
    "    test_size=0.2,        # 20% for testing\n",
    "    random_state=42,      # ensures same split every run\n",
    "    stratify=y_selected   # keeps class distribution consistent\n",
    ")\n",
    "\n",
    "print(\"\\nTrain/Test Split Summary:\")\n",
    "print(\"Training samples:\", X_train.shape[0])\n",
    "print(\"Testing samples:\", X_test.shape[0])\n",
    "print(\"Number of features used:\", X_train.shape[1])\n",
    "\n",
    "print(\"\\nTraining class distribution:\")\n",
    "print(y_train.value_counts())\n",
    "\n",
    "print(\"\\nTesting class distribution:\")\n",
    "print(y_test.value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c3490e8",
   "metadata": {},
   "source": [
    "### Step 5 — Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3892bcdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Feature Scaling Completed.\n",
      "Scaled Training Data Shape: (455, 5)\n",
      "Scaled Testing Data Shape: (114, 5)\n",
      "\n",
      "Mean of first scaled feature (train): -4.489693110572061e-16\n",
      "Std of first scaled feature (train): 1.0000000000000004\n"
     ]
    }
   ],
   "source": [
    "# =============================\n",
    "# 5. Feature Scaling\n",
    "# =============================\n",
    "\n",
    "\"\"\"\n",
    "Goal:\n",
    "Standardize the feature values.\n",
    "\n",
    "Why scaling is important?\n",
    "SVM is a distance-based algorithm.\n",
    "Features with larger numeric ranges can dominate smaller ones.\n",
    "Therefore, we standardize features to have:\n",
    "    - Mean = 0\n",
    "    - Standard deviation = 1\n",
    "\n",
    "Important:\n",
    "We fit the scaler ONLY on training data.\n",
    "Then we transform both training and testing data.\n",
    "This avoids data leakage.\n",
    "\"\"\"\n",
    "\n",
    "# Initialize scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit only on training data\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "# Use same transformation on test data\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"\\nFeature Scaling Completed.\")\n",
    "print(\"Scaled Training Data Shape:\", X_train_scaled.shape)\n",
    "print(\"Scaled Testing Data Shape:\", X_test_scaled.shape)\n",
    "\n",
    "# Optional sanity check: mean and std of training set\n",
    "print(\"\\nMean of first scaled feature (train):\", np.mean(X_train_scaled[:, 0]))\n",
    "print(\"Std of first scaled feature (train):\", np.std(X_train_scaled[:, 0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5706848",
   "metadata": {},
   "source": [
    "### Step 6 — Train SVM with Multiple Kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "643e466e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training SVM models with different kernels...\n",
      "\n",
      "Kernel 'linear' training completed.\n",
      "Kernel 'poly' training completed.\n",
      "Kernel 'rbf' training completed.\n",
      "Kernel 'sigmoid' training completed.\n"
     ]
    }
   ],
   "source": [
    "# =============================\n",
    "# 6. SVM Model Training (Multiple Kernels)\n",
    "# =============================\n",
    "\n",
    "\"\"\"\n",
    "Goal:\n",
    "Train SVM models using different kernel functions.\n",
    "\n",
    "We will test:\n",
    "    - Linear\n",
    "    - Polynomial\n",
    "    - RBF (Gaussian)\n",
    "    - Sigmoid\n",
    "\n",
    "For each kernel:\n",
    "    1. Train the model\n",
    "    2. Make predictions on test set\n",
    "    3. Store predictions for evaluation\n",
    "\"\"\"\n",
    "\n",
    "# Define kernels to test\n",
    "kernels = [\"linear\", \"poly\", \"rbf\", \"sigmoid\"]\n",
    "\n",
    "# Dictionary to store predictions for each kernel\n",
    "predictions = {}\n",
    "\n",
    "print(\"\\nTraining SVM models with different kernels...\\n\")\n",
    "\n",
    "for kernel in kernels:\n",
    "    \n",
    "    # Initialize SVM model with current kernel\n",
    "    model = SVC(kernel=kernel, random_state=42)\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Predict on test data\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    \n",
    "    # Store predictions\n",
    "    predictions[kernel] = y_pred\n",
    "    \n",
    "    print(f\"Kernel '{kernel}' training completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3322bbf3",
   "metadata": {},
   "source": [
    "### Step 7 — Performance Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6a0b917e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating Models...\n",
      "\n",
      "Kernel 'linear' evaluation completed.\n",
      "Kernel 'poly' evaluation completed.\n",
      "Kernel 'rbf' evaluation completed.\n",
      "Kernel 'sigmoid' evaluation completed.\n",
      "\n",
      "Model Performance Comparison:\n",
      "    Kernel  Accuracy  Precision    Recall  F1_Score\n",
      "0   linear  0.947368   0.971429  0.944444  0.957746\n",
      "1     poly  0.912281   0.878049  1.000000  0.935065\n",
      "2      rbf  0.938596   0.971014  0.930556  0.950355\n",
      "3  sigmoid  0.912281   0.955882  0.902778  0.928571\n",
      "\n",
      "Metrics saved to: results\\svm_kernel_comparison.csv\n",
      "Confusion matrices saved inside 'results/' folder.\n"
     ]
    }
   ],
   "source": [
    "# =============================\n",
    "# 7. Performance Evaluation\n",
    "# =============================\n",
    "\n",
    "\"\"\"\n",
    "Goal:\n",
    "Evaluate performance of each SVM kernel using:\n",
    "\n",
    "    - Confusion Matrix (saved as PNG)\n",
    "    - Accuracy\n",
    "    - Precision\n",
    "    - Recall\n",
    "    - F1 Score\n",
    "\n",
    "All metrics will be saved into a CSV file.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Create output directory if not exists\n",
    "output_dir = \"results\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# List to store metric results\n",
    "results = []\n",
    "\n",
    "print(\"\\nEvaluating Models...\\n\")\n",
    "\n",
    "for kernel in kernels:\n",
    "    \n",
    "    y_pred = predictions[kernel]\n",
    "    \n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    # Compute metrics\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    prec = precision_score(y_test, y_pred)\n",
    "    rec = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    \n",
    "    # Save metrics in results list\n",
    "    results.append({\n",
    "        \"Kernel\": kernel,\n",
    "        \"Accuracy\": acc,\n",
    "        \"Precision\": prec,\n",
    "        \"Recall\": rec,\n",
    "        \"F1_Score\": f1\n",
    "    })\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    plt.figure(figsize=(5,4))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "    plt.title(f\"Confusion Matrix - {kernel.upper()} Kernel\")\n",
    "    plt.xlabel(\"Predicted Label\")\n",
    "    plt.ylabel(\"True Label\")\n",
    "    \n",
    "    # Save figure\n",
    "    plt.savefig(os.path.join(output_dir, f\"confusion_matrix_{kernel}.png\"))\n",
    "    plt.close()\n",
    "    \n",
    "    print(f\"Kernel '{kernel}' evaluation completed.\")\n",
    "\n",
    "\n",
    "# Convert results to DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Display comparison table\n",
    "print(\"\\nModel Performance Comparison:\")\n",
    "print(results_df)\n",
    "\n",
    "# Save results to CSV\n",
    "csv_path = os.path.join(output_dir, \"svm_kernel_comparison.csv\")\n",
    "results_df.to_csv(csv_path, index=False)\n",
    "\n",
    "print(f\"\\nMetrics saved to: {csv_path}\")\n",
    "print(\"Confusion matrices saved inside 'results/' folder.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b849da9",
   "metadata": {},
   "source": [
    "### Step 8 — Conclusion Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3870e510",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=============================\n",
      "FINAL CONCLUSION\n",
      "=============================\n",
      "\n",
      "Best Performing Kernel: LINEAR\n",
      "Highest Accuracy Achieved: 0.9474\n",
      "\n",
      "Detailed Performance of Best Model:\n",
      "Precision: 0.9714\n",
      "Recall: 0.9444\n",
      "F1 Score: 0.9577\n",
      "\n",
      "Interpretation:\n",
      "The kernel with the highest accuracy is considered the most suitable\n",
      "for this dataset using the selected features.\n",
      "Different kernels capture different decision boundaries.\n",
      "Linear kernel performs well when data is approximately linearly separable,\n",
      "while RBF and Polynomial kernels handle more complex boundaries.\n",
      "The results suggest that the selected features provide strong\n",
      "discriminative power for breast cancer classification.\n"
     ]
    }
   ],
   "source": [
    "# =============================\n",
    "# 8. Conclusion\n",
    "# =============================\n",
    "\n",
    "\"\"\"\n",
    "Goal:\n",
    "Interpret the performance results and identify the best-performing kernel.\n",
    "\"\"\"\n",
    "\n",
    "# Identify best kernel based on highest accuracy\n",
    "best_model = results_df.loc[results_df[\"Accuracy\"].idxmax()]\n",
    "\n",
    "best_kernel = best_model[\"Kernel\"]\n",
    "best_accuracy = best_model[\"Accuracy\"]\n",
    "\n",
    "print(\"\\n=============================\")\n",
    "print(\"FINAL CONCLUSION\")\n",
    "print(\"=============================\")\n",
    "\n",
    "print(f\"\\nBest Performing Kernel: {best_kernel.upper()}\")\n",
    "print(f\"Highest Accuracy Achieved: {best_accuracy:.4f}\")\n",
    "\n",
    "print(\"\\nDetailed Performance of Best Model:\")\n",
    "print(f\"Precision: {best_model['Precision']:.4f}\")\n",
    "print(f\"Recall: {best_model['Recall']:.4f}\")\n",
    "print(f\"F1 Score: {best_model['F1_Score']:.4f}\")\n",
    "\n",
    "print(\"\\nInterpretation:\")\n",
    "print(\"The kernel with the highest accuracy is considered the most suitable\")\n",
    "print(\"for this dataset using the selected features.\")\n",
    "print(\"Different kernels capture different decision boundaries.\")\n",
    "print(\"Linear kernel performs well when data is approximately linearly separable,\")\n",
    "print(\"while RBF and Polynomial kernels handle more complex boundaries.\")\n",
    "print(\"The results suggest that the selected features provide strong\")\n",
    "print(\"discriminative power for breast cancer classification.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d6aafa",
   "metadata": {},
   "source": [
    "**Results Interpretation**\n",
    "\n",
    "In this investigation, Support Vector Machine (SVM) models were evaluated using four different kernel functions: Linear, Polynomial, RBF, and Sigmoid. Five most relevant features were selected based on highest correlation with the target variable to reduce dimensionality while preserving discriminative information. Among the tested kernels, the Linear kernel achieved the highest accuracy (94.74%), along with strong Precision (97.14%), Recall (94.44%), and F1-score (95.77%). This suggests that the selected features allow the data to be approximately linearly separable in the reduced feature space. Although RBF and Polynomial kernels also performed competitively, the Linear kernel provided the best overall balance of performance metrics for this dataset Therefore, for the selected features and dataset configuration, the Linear SVM is the most suitable model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
